---
title: yoloで物体検知をやってみる
tags:
  - Python
  - macmini
  - 物体検知
  - uv(astral-sh)
  - yolo11
private: false
updated_at: '2025-02-03T22:25:02+09:00'
id: 7205c71387679e265aad
organization_url_name: null
slide: false
ignorePublish: false
---
# あいさつ
Qiita初投稿になるためどんな感じかわかりませんが、先人の真似をとりあえずしていこうと思います。普段はバックエンド中心のWebエンジニアでpythonとRustを使っています。たまにNextjsでフロントエンドもするぐらいです。  
エンジニアとしては、もう少しで2年になるペーペーです。お手柔らかにお願いいたします。
ちなみに最近初のmacosとなるMac miniを買いました。今までWindows一筋だったので、キー配置が違和感でしかありません。

# はじめに
今働いている会社は物体検知とかもするにも関わらず私はやったことありません。なのでどんなもんかと思いやってみようと思います。
全くわからないと言いつつYoloというのは聞いたことがあります。最新はYolo11なのでそちらを使ってみようと思います！

https://docs.ultralytics.com/ja/models/yolo11/

# 環境
私の環境はこんな感じでやります。

| 項目  | 詳細 |
|------|------|
| **OS** | macOS 15.2 |
| **CPU** | Apple M4 Pro |
| **メモリ** | 24 GB |
| **GPU** | Apple M4 Pro (16コア) |

<br>

| 項目  | 詳細 |
|------|------|
| **Python バージョン** | 3.12.8 |
| **バージョン管理ツール** | uv |

Rustも使っていることもありuvに溺愛しています。

# いざ物体検知！

まずは環境構築と必要なもののinstall。
```bash
uv init yolo-test
uv python install 3.12
uv python pin 3.12
uv add ultralytics opencv-python
```
やっぱ少ないコマンドで諸々セットアップできるの最高。

ちなみに3.13.1tでinstallしようとしたらエラーでました。

copilotの力を使ってコードを書いてみる。

```py:main.py
import cv2
from ultralytics import YOLO

def main():
    
    model = YOLO("yolo11n.pt")
    
    # set the input and output video paths
    input_video_path = "data/input_video.mp4"
    output_video_path = "data/output_video.mp4"
    
    # read the input video information
    cap = cv2.VideoCapture(input_video_path)
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    
    print(f"video information: fps={fps}, width={width}, height={height}")
    
    # create the output video writer
    out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))
    
    # read the video frame by frame
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        
        # rotate the frame
        frame = cv2.rotate(frame, cv2.ROTATE_180)
        
        # detect objects in the frame
        result_list = model(frame)
        
        # draw the bounding boxes
        for result in result_list:
            frame = result.plot()
        
        # write the frame to the output video
        out.write(frame)
        
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
        
    cap.release()
    out.release()
    
    print("Done!")

if __name__ == "__main__":
    main()

```

動画が勝手に反転するので、直す処理を入れている。
```py
# rotate the frame
frame = cv2.rotate(frame, cv2.ROTATE_180)
```

あとはinputの動画を準備する。ちょうどスマホに愛犬とルンバの戦いがあったのでそれを使う。

いざ実行してみると、、、
```bash
uv run main.py
video information: fps=30, width=1280, height=720

0: 384x640 1 dog, 24.8ms
Speed: 1.4ms preprocess, 24.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)

0: 384x640 1 person, 1 dog, 18.0ms
Speed: 0.7ms preprocess, 18.0ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)

```
動いている！

![output_video_2.gif](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3722589/70052756-e62f-3e45-3fab-2ae2f72a6728.gif)

ルンバはボウルと判別されるんだな🧐

# 最後に
以外と簡単だった。いつかペットカメラにして寝てる時間とか計測したいなと思っています。
これからも適度に投稿しようかなと思うのでよろしくお願いいたします！（もっとためになるものを投稿したい！）
週１ぐらいで投稿しようかな！
